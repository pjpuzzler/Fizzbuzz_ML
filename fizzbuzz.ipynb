{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BINARY_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(num):\n",
    "    return bin(num)[2:]\n",
    "\n",
    "def get_fizzbuzz(num):\n",
    "    if num % 15 == 0:\n",
    "        return \"fizzbuzz\"\n",
    "    if num % 5 == 0:\n",
    "        return \"buzz\"\n",
    "    if num % 3 == 0:\n",
    "        return \"fizz\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[int(bit) for bit in binary(num).zfill(MAX_BINARY_LEN)] for num in range(101, 2 ** 10)])\n",
    "y_train = np.array([get_fizzbuzz(num) for num in range(101, 2 ** 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[int(bit) for bit in binary(num).zfill(MAX_BINARY_LEN)] for num in range(1, 101)])\n",
    "y_test = np.array([get_fizzbuzz(num) for num in range(1, 101)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier((400,), learning_rate_init=0.005, max_iter=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.20724475\n",
      "Iteration 2, loss = 1.18067735\n",
      "Iteration 3, loss = 1.15399277\n",
      "Iteration 4, loss = 1.14274834\n",
      "Iteration 5, loss = 1.13611250\n",
      "Iteration 6, loss = 1.13484151\n",
      "Iteration 7, loss = 1.12763719\n",
      "Iteration 8, loss = 1.12455083\n",
      "Iteration 9, loss = 1.11981743\n",
      "Iteration 10, loss = 1.11324691\n",
      "Iteration 11, loss = 1.10549607\n",
      "Iteration 12, loss = 1.09723986\n",
      "Iteration 13, loss = 1.08778355\n",
      "Iteration 14, loss = 1.07764662\n",
      "Iteration 15, loss = 1.06532700\n",
      "Iteration 16, loss = 1.05301873\n",
      "Iteration 17, loss = 1.04124696\n",
      "Iteration 18, loss = 1.02772229\n",
      "Iteration 19, loss = 1.01514417\n",
      "Iteration 20, loss = 0.99650773\n",
      "Iteration 21, loss = 0.97972121\n",
      "Iteration 22, loss = 0.96170533\n",
      "Iteration 23, loss = 0.94331867\n",
      "Iteration 24, loss = 0.92337082\n",
      "Iteration 25, loss = 0.90458528\n",
      "Iteration 26, loss = 0.88501770\n",
      "Iteration 27, loss = 0.86295849\n",
      "Iteration 28, loss = 0.84231408\n",
      "Iteration 29, loss = 0.82289146\n",
      "Iteration 30, loss = 0.80073873\n",
      "Iteration 31, loss = 0.77985661\n",
      "Iteration 32, loss = 0.75868033\n",
      "Iteration 33, loss = 0.73665387\n",
      "Iteration 34, loss = 0.71744993\n",
      "Iteration 35, loss = 0.69397615\n",
      "Iteration 36, loss = 0.67668918\n",
      "Iteration 37, loss = 0.65595489\n",
      "Iteration 38, loss = 0.63685840\n",
      "Iteration 39, loss = 0.61858508\n",
      "Iteration 40, loss = 0.59462822\n",
      "Iteration 41, loss = 0.57272698\n",
      "Iteration 42, loss = 0.54982693\n",
      "Iteration 43, loss = 0.53302729\n",
      "Iteration 44, loss = 0.51570372\n",
      "Iteration 45, loss = 0.49437653\n",
      "Iteration 46, loss = 0.47239056\n",
      "Iteration 47, loss = 0.45507268\n",
      "Iteration 48, loss = 0.43716410\n",
      "Iteration 49, loss = 0.42379537\n",
      "Iteration 50, loss = 0.40466361\n",
      "Iteration 51, loss = 0.39144559\n",
      "Iteration 52, loss = 0.37653859\n",
      "Iteration 53, loss = 0.36515746\n",
      "Iteration 54, loss = 0.35147555\n",
      "Iteration 55, loss = 0.33201923\n",
      "Iteration 56, loss = 0.31761985\n",
      "Iteration 57, loss = 0.30500373\n",
      "Iteration 58, loss = 0.29349662\n",
      "Iteration 59, loss = 0.28229516\n",
      "Iteration 60, loss = 0.26989176\n",
      "Iteration 61, loss = 0.26062972\n",
      "Iteration 62, loss = 0.25016050\n",
      "Iteration 63, loss = 0.23956303\n",
      "Iteration 64, loss = 0.22924861\n",
      "Iteration 65, loss = 0.21991089\n",
      "Iteration 66, loss = 0.21129222\n",
      "Iteration 67, loss = 0.20353902\n",
      "Iteration 68, loss = 0.19617969\n",
      "Iteration 69, loss = 0.18894705\n",
      "Iteration 70, loss = 0.18486080\n",
      "Iteration 71, loss = 0.17633626\n",
      "Iteration 72, loss = 0.16981565\n",
      "Iteration 73, loss = 0.16550265\n",
      "Iteration 74, loss = 0.15890232\n",
      "Iteration 75, loss = 0.15379592\n",
      "Iteration 76, loss = 0.14809221\n",
      "Iteration 77, loss = 0.14173252\n",
      "Iteration 78, loss = 0.13671361\n",
      "Iteration 79, loss = 0.13239387\n",
      "Iteration 80, loss = 0.12799116\n",
      "Iteration 81, loss = 0.12406448\n",
      "Iteration 82, loss = 0.12044617\n",
      "Iteration 83, loss = 0.11815792\n",
      "Iteration 84, loss = 0.11263046\n",
      "Iteration 85, loss = 0.10832350\n",
      "Iteration 86, loss = 0.10485126\n",
      "Iteration 87, loss = 0.10326513\n",
      "Iteration 88, loss = 0.09879890\n",
      "Iteration 89, loss = 0.09635336\n",
      "Iteration 90, loss = 0.09253815\n",
      "Iteration 91, loss = 0.09022377\n",
      "Iteration 92, loss = 0.08716162\n",
      "Iteration 93, loss = 0.08512051\n",
      "Iteration 94, loss = 0.08320805\n",
      "Iteration 95, loss = 0.08049153\n",
      "Iteration 96, loss = 0.07808131\n",
      "Iteration 97, loss = 0.07645763\n",
      "Iteration 98, loss = 0.07476915\n",
      "Iteration 99, loss = 0.07358228\n",
      "Iteration 100, loss = 0.07217032\n",
      "Iteration 101, loss = 0.06947198\n",
      "Iteration 102, loss = 0.06707879\n",
      "Iteration 103, loss = 0.06606965\n",
      "Iteration 104, loss = 0.06420144\n",
      "Iteration 105, loss = 0.06217395\n",
      "Iteration 106, loss = 0.06152977\n",
      "Iteration 107, loss = 0.05974559\n",
      "Iteration 108, loss = 0.05849811\n",
      "Iteration 109, loss = 0.05693634\n",
      "Iteration 110, loss = 0.05572524\n",
      "Iteration 111, loss = 0.05408177\n",
      "Iteration 112, loss = 0.05276625\n",
      "Iteration 113, loss = 0.05168153\n",
      "Iteration 114, loss = 0.05066642\n",
      "Iteration 115, loss = 0.04935795\n",
      "Iteration 116, loss = 0.04838222\n",
      "Iteration 117, loss = 0.04767528\n",
      "Iteration 118, loss = 0.04671418\n",
      "Iteration 119, loss = 0.04539506\n",
      "Iteration 120, loss = 0.04443078\n",
      "Iteration 121, loss = 0.04333209\n",
      "Iteration 122, loss = 0.04258911\n",
      "Iteration 123, loss = 0.04173145\n",
      "Iteration 124, loss = 0.04128777\n",
      "Iteration 125, loss = 0.04024771\n",
      "Iteration 126, loss = 0.03899044\n",
      "Iteration 127, loss = 0.03898480\n",
      "Iteration 128, loss = 0.03786878\n",
      "Iteration 129, loss = 0.03742020\n",
      "Iteration 130, loss = 0.03621993\n",
      "Iteration 131, loss = 0.03592182\n",
      "Iteration 132, loss = 0.03536606\n",
      "Iteration 133, loss = 0.03452489\n",
      "Iteration 134, loss = 0.03406118\n",
      "Iteration 135, loss = 0.03322290\n",
      "Iteration 136, loss = 0.03256903\n",
      "Iteration 137, loss = 0.03205617\n",
      "Iteration 138, loss = 0.03135837\n",
      "Iteration 139, loss = 0.03104543\n",
      "Iteration 140, loss = 0.03066229\n",
      "Iteration 141, loss = 0.03014582\n",
      "Iteration 142, loss = 0.02989005\n",
      "Iteration 143, loss = 0.02922438\n",
      "Iteration 144, loss = 0.02819069\n",
      "Iteration 145, loss = 0.02797102\n",
      "Iteration 146, loss = 0.02819934\n",
      "Iteration 147, loss = 0.02760201\n",
      "Iteration 148, loss = 0.02714122\n",
      "Iteration 149, loss = 0.02646132\n",
      "Iteration 150, loss = 0.02606942\n",
      "Iteration 151, loss = 0.02547195\n",
      "Iteration 152, loss = 0.02599460\n",
      "Iteration 153, loss = 0.02572557\n",
      "Iteration 154, loss = 0.02499374\n",
      "Iteration 155, loss = 0.02461798\n",
      "Iteration 156, loss = 0.02465511\n",
      "Iteration 157, loss = 0.02455343\n",
      "Iteration 158, loss = 0.02324035\n",
      "Iteration 159, loss = 0.02296744\n",
      "Iteration 160, loss = 0.02231637\n",
      "Iteration 161, loss = 0.02198614\n",
      "Iteration 162, loss = 0.02141934\n",
      "Iteration 163, loss = 0.02103162\n",
      "Iteration 164, loss = 0.02066352\n",
      "Iteration 165, loss = 0.02050979\n",
      "Iteration 166, loss = 0.02026546\n",
      "Iteration 167, loss = 0.02031277\n",
      "Iteration 168, loss = 0.01990988\n",
      "Iteration 169, loss = 0.01980447\n",
      "Iteration 170, loss = 0.01932766\n",
      "Iteration 171, loss = 0.01889489\n",
      "Iteration 172, loss = 0.01859005\n",
      "Iteration 173, loss = 0.01851014\n",
      "Iteration 174, loss = 0.01823456\n",
      "Iteration 175, loss = 0.01781858\n",
      "Iteration 176, loss = 0.01758493\n",
      "Iteration 177, loss = 0.01732128\n",
      "Iteration 178, loss = 0.01710058\n",
      "Iteration 179, loss = 0.01681161\n",
      "Iteration 180, loss = 0.01667660\n",
      "Iteration 181, loss = 0.01639212\n",
      "Iteration 182, loss = 0.01646769\n",
      "Iteration 183, loss = 0.01628088\n",
      "Iteration 184, loss = 0.01673543\n",
      "Iteration 185, loss = 0.01616352\n",
      "Iteration 186, loss = 0.01563485\n",
      "Iteration 187, loss = 0.01535584\n",
      "Iteration 188, loss = 0.01509922\n",
      "Iteration 189, loss = 0.01484657\n",
      "Iteration 190, loss = 0.01474647\n",
      "Iteration 191, loss = 0.01483038\n",
      "Iteration 192, loss = 0.01450030\n",
      "Iteration 193, loss = 0.01428443\n",
      "Iteration 194, loss = 0.01411259\n",
      "Iteration 195, loss = 0.01407079\n",
      "Iteration 196, loss = 0.01383216\n",
      "Iteration 197, loss = 0.01364620\n",
      "Iteration 198, loss = 0.01339729\n",
      "Iteration 199, loss = 0.01332359\n",
      "Iteration 200, loss = 0.01312954\n",
      "Iteration 201, loss = 0.01299957\n",
      "Iteration 202, loss = 0.01285127\n",
      "Iteration 203, loss = 0.01274033\n",
      "Iteration 204, loss = 0.01285534\n",
      "Iteration 205, loss = 0.01277151\n",
      "Iteration 206, loss = 0.01261661\n",
      "Iteration 207, loss = 0.01240967\n",
      "Iteration 208, loss = 0.01225533\n",
      "Iteration 209, loss = 0.01211970\n",
      "Iteration 210, loss = 0.01183648\n",
      "Iteration 211, loss = 0.01162965\n",
      "Iteration 212, loss = 0.01156300\n",
      "Iteration 213, loss = 0.01142492\n",
      "Iteration 214, loss = 0.01118805\n",
      "Iteration 215, loss = 0.01107712\n",
      "Iteration 216, loss = 0.01108060\n",
      "Iteration 217, loss = 0.01086311\n",
      "Iteration 218, loss = 0.01092071\n",
      "Iteration 219, loss = 0.01079379\n",
      "Iteration 220, loss = 0.01064571\n",
      "Iteration 221, loss = 0.01065797\n",
      "Iteration 222, loss = 0.01048202\n",
      "Iteration 223, loss = 0.01027945\n",
      "Iteration 224, loss = 0.01016146\n",
      "Iteration 225, loss = 0.01006273\n",
      "Iteration 226, loss = 0.00996220\n",
      "Iteration 227, loss = 0.00987121\n",
      "Iteration 228, loss = 0.00974211\n",
      "Iteration 229, loss = 0.00966261\n",
      "Iteration 230, loss = 0.00958145\n",
      "Iteration 231, loss = 0.00944161\n",
      "Iteration 232, loss = 0.00936589\n",
      "Iteration 233, loss = 0.00924463\n",
      "Iteration 234, loss = 0.00926778\n",
      "Iteration 235, loss = 0.00916437\n",
      "Iteration 236, loss = 0.00921679\n",
      "Iteration 237, loss = 0.00899180\n",
      "Iteration 238, loss = 0.00891118\n",
      "Iteration 239, loss = 0.00876671\n",
      "Iteration 240, loss = 0.00868194\n",
      "Iteration 241, loss = 0.00863000\n",
      "Iteration 242, loss = 0.00854778\n",
      "Iteration 243, loss = 0.00844831\n",
      "Iteration 244, loss = 0.00835936\n",
      "Iteration 245, loss = 0.00835416\n",
      "Iteration 246, loss = 0.00833482\n",
      "Iteration 247, loss = 0.00827026\n",
      "Iteration 248, loss = 0.00823454\n",
      "Iteration 249, loss = 0.00804169\n",
      "Iteration 250, loss = 0.00798169\n",
      "Iteration 251, loss = 0.00790202\n",
      "Iteration 252, loss = 0.00785854\n",
      "Iteration 253, loss = 0.00785411\n",
      "Iteration 254, loss = 0.00775971\n",
      "Iteration 255, loss = 0.00773531\n",
      "Iteration 256, loss = 0.00759470\n",
      "Iteration 257, loss = 0.00748734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.00740812\n",
      "Iteration 259, loss = 0.00735514\n",
      "Iteration 260, loss = 0.00732016\n",
      "Iteration 261, loss = 0.00723988\n",
      "Iteration 262, loss = 0.00719964\n",
      "Iteration 263, loss = 0.00712417\n",
      "Iteration 264, loss = 0.00708998\n",
      "Iteration 265, loss = 0.00700394\n",
      "Iteration 266, loss = 0.00695701\n",
      "Iteration 267, loss = 0.00692037\n",
      "Iteration 268, loss = 0.00684417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(400,), learning_rate_init=0.005,\n",
       "              max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 \n",
      "3 fizz\n",
      "4 \n",
      "5 buzz\n",
      "6 fizz\n",
      "7 \n",
      "8 \n",
      "9 fizz\n",
      "10 buzz\n",
      "11 \n",
      "12 fizz\n",
      "13 \n",
      "14 \n",
      "15 fizzbuzz\n",
      "16 \n",
      "17 \n",
      "18 fizz\n",
      "19 \n",
      "20 buzz\n",
      "21 fizz\n",
      "22 \n",
      "23 \n",
      "24 fizz\n",
      "25 buzz\n",
      "26 \n",
      "27 fizz\n",
      "28 \n",
      "29 \n",
      "30 fizzbuzz\n",
      "31 \n",
      "32 \n",
      "33 fizz\n",
      "34 \n",
      "35 buzz\n",
      "36 fizz\n",
      "37 \n",
      "38 \n",
      "39 fizz\n",
      "40 buzz\n",
      "41 \n",
      "42 fizz\n",
      "43 \n",
      "44 \n",
      "45 fizzbuzz\n",
      "46 \n",
      "47 \n",
      "48 fizz\n",
      "49 \n",
      "50 buzz\n",
      "51 fizz\n",
      "52 \n",
      "53 \n",
      "54 fizz\n",
      "55 buzz\n",
      "56 \n",
      "57 fizz\n",
      "58 \n",
      "59 \n",
      "60 fizzbuzz\n",
      "61 \n",
      "62 \n",
      "63 fizz\n",
      "64 \n",
      "65 buzz\n",
      "66 fizz\n",
      "67 \n",
      "68 \n",
      "69 fizz\n",
      "70 buzz\n",
      "71 \n",
      "72 fizz\n",
      "73 \n",
      "74 \n",
      "75 fizzbuzz\n",
      "76 \n",
      "77 \n",
      "78 fizz\n",
      "79 \n",
      "80 buzz\n",
      "81 fizz\n",
      "82 \n",
      "83 \n",
      "84 fizz\n",
      "85 buzz\n",
      "86 \n",
      "87 fizz\n",
      "88 \n",
      "89 \n",
      "90 fizzbuzz\n",
      "91 \n",
      "92 \n",
      "93 fizz\n",
      "94 \n",
      "95 buzz\n",
      "96 fizz\n",
      "97 \n",
      "98 \n",
      "99 fizz\n",
      "100 buzz\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 101):\n",
    "    print(i, clf.predict(np.array([int(bit) for bit in binary(i).zfill(MAX_BINARY_LEN)]).reshape(1, -1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
